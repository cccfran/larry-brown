[
  {
    "id": 1,
    "speakers": [
      {
        "name": "Sarah Mayner",
        "class": "WG21",
        "pic": "Sarah_Mayner.jpg",
        "linkedin" : "https://www.linkedin.com/in/sarah-mayner/"
      },
      {
        "name": "Mit Shah",
        "class": "WG21",
        "pic": "Mit_Shah.jpg"
      },
      {
        "name": "Laural Knebel",
        "class": "WG21",
        "pic": "Laura_Knebel.jpg"
      }
    ],
    "time": "",
    "title": "Who's watching you? An analysis of mobile app trackers",
    "abstract": "Over the years, the number of third party trackers embedded in mobile applications (App store/Google Play store) has exploded. These trackers not only track users' online activity and behavior, but also offline movements and activity. The goal of our project is to identify categories of apps that collect an unnecessary amount of data on users. Our hope is that this work will allow privacy-conscious smartphone users to make more informed app choices and that it may enlighten future policy decisions.\n\nFirst, we will analyze the volume and type of trackers installed across several app categories. (Our project builds on analysis conducted by Oxford researchers, who have made tracker data by app category publicly available for >1 million applications.) Next, we will categorize the trackers' range of activity using a previously established mapping. We will then create a framework to define the range of acceptable tracking activity for each app category, paying particular attention to data deemed sensitive by Google and/or not within the scope of the app's core functionality. We will then apply this framework to determine which app categories collect more data on users than necessary."
  },
  {
    "id": 2,
    "speakers": [
      {
        "name": "Shaolong Wu",
        "class": "",
        "pic": null,
        "linkedin" : null
      },
      {
        "name": "Yuzhou Lin",
        "class": "",
        "pic": null
      },
      {
        "name": "Lingqi Zhang",
        "class": "",
        "pic": null
      }
    ],
    "time": "",
    "title": "Convolutional Neural Network Decoding from Visual Cortex",
    "abstract": "Advance in techniques for measuring brain activities, such as functional Magnetic Resonance Imaging (fMRI), has allowed for valuable insight into how neural activities is connected to our behavior. Previously it has been shown, by building forward models of visual cortical responses to natural images [1], researchers can achieve “mind-reading”, identifying the images viewed by human subjects directly from their neural responses.\n\nOur project proposes to extend their original analysis with the recent advances in deep neural networks, which has been the gold standard for image processing related tasks [2]. The specific aims for our projects are the following: \n\n\t1. Use the original data from [1], first perform exploratory data analysis, and use traditional methods, to help us better understand the function and organization of visual cortex. \n\n2.Build and train a convolution neural network that aims to predict the pattern of brain responses, given natural images. With a properly trained and regularized network, we should be able to outperform the original method, which uses hand-designed feature. \n\n3.Our most ambitious goal would be to reconstruct the images viewed by human subjects from their brain activities using a Bayesian algorithm [3], again leveraging recent advances in machine learning techniques, in particular models of natural images [4]. \n\nOur project will help gain additional insight into the mechanism of visual system, which will be important for both build system of computer vision, and cure diseases related to vision. \n\n[1] Kay, K. N., Naselaris, T., Prenger, R. J., & Gallant, J. L. (2008). Identifying natural images from human brain activity. Nature, 452(7185), 352-355.\n[2] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.\n [3] Naselaris, T., Prenger, R. J., Kay, K. N., Oliver, M., & Gallant, J. L. (2009). Bayesian reconstruction of natural images from human brain activity. Neuron, 63(6), 902-915.\n[4] Kadkhodaie, Z., & Simoncelli, E. P. (2020). Solving linear inverse problems using the prior implicit in a denoiser. arXiv preprint arXiv:2007.13640."
  },
  {
    "id": 3,
    "speakers": [
      {
        "name": "Alex Coble",
        "class": "PhD",
        "pic": null,
        "linkedin" : ""
      },
      {
        "name": "Joe Moran",
        "class": "PhD",
        "pic": null
      }
    ],
    "time": "",
    "title": "Real Effects of Disclosure: Evidence from Universities’ COVID-19 Dashboard",
    "abstract": "An important issue in accounting research is whether firms’ disclosures, or more broadly, information dissemination, have an impact on individuals’ behavior. One important source of information for college students during the COVID-19 pandemic has been their schools’ COVID-19 dashboards. It is common for schools to provide a live tracker containing information about case rates, testing, vaccinations, policies, etc. (Penn’s dashboard is linked here); these trackers are considered disclosures in the accounting field as they provide information to stakeholders about schools’ performance in fighting the pandemic. The question we seek to answer is: do schools’ disclosures via their COVID-19 dashboards have a real impact on COVID transmission rates?\n\nUsing HTML data scraped from schools’ COVID-19 dashboards over the Fall 2020 semester using the Wayback Machine, we will analyze changes in COVID transmission rates  (“case rates”) in schools’ counties during the week(s) following schools’ disclosures via their dashboards. We intend to use the rvest and xml2 packages to download and parse dashboards to generate measures of their content (e.g., number of words, number of HTML formatting tags, presence of multimedia, etc.); we will then predict changes in county-level case rate growth using changes in dashboard content. While we note our study will not have a causal interpretation, we intend to provide evidence supporting the hypothesis that schools’ disclosures have real effects by using a difference-in-differences design comparing counties with disclosing schools vs. counties without them, investigating cross-sectional differences in schools’ dashboard quality and their effect on case rate growth, and investigating schools’ change in disclosure behavior after receiving a dashboard rating from an external party and its effect on case growth."
  },
  {
    "id": 4,
    "speakers": [
      {
        "name": "Claire Allen-Platt",
        "class": "PhD",
        "pic": "claire_allen-platt.jpg",
        "linkedin" : "https://www.linkedin.com/in/claireallenplatt/"
      }
    ],
    "time": "",
    "title": "Predicting educational opportunity: a methods comparison using population data",
    "abstract": "Reardon (2019) argues that the average standardized test score in a school or district “can be thought of as reflecting the average cumulative set of educational opportunities children in a community have had up to the time when they take a test” (p. 41). The following analysis identifies predictors of average achievement for the population of public elementary and middle schools in the United States. Statistical methods that predict school-level achievement as a binary outcome (above/below the national average), an ordinal outcome (quartiles of achievement relative to a national average), and a continuous outcome (standard deviation units, referenced to a nationwide average) are compared. The goal is to describe methods well-suited to investigating hypotheses about educational opportunity using data that are population level, public use, and aggregate. Data come from the Stanford Education Data Archive (SEDA), which computes a single, pooled achievement estimate across tested grades (3-8), tested subjects (reading and math), and time (10 years, academic years 2008-09 to 2017-18), and places it on a nationally comparable scale, for elementary and middle schools in every public school district in the United States. Candidate predictor variables come from public-use datasets collected by the federal government: the US Census; the American Community Survey; the Civil Rights Data Collection; the School Attendance Boundary Information System, and more, and measure characteristics of schools’ students, teachers, finances, opportunities to learn, discipline practices, neighborhoods, and communities.\n\nReferences: Reardon, S. (2019). Educational opportunity in early and middle childhood: Using full population administrative data to study variation by place and age. <em>RSF: The Russell Sage Foundation Journal of the Social Sciences</em> 5(2), pp. 40-68."
  },
  {
    "id": 5,
    "speakers": [
      {
        "name": "Keana Richards",
        "class": "",
        "pic": null,
        "linkedin" : ""
      }
    ],
    "time": "",
    "title": "Eat Healthy",
    "abstract": "<strong>Background</strong><br>Healthy eating is fundamental to good health at every stage of life. Many countries around the world are providing nationwide dietary guidelines to the public and requiring nutrition facts labels on food products to help people make informed choices about foods and drinks that contain nutrients that they may want more or less of. Every individual, however, has different nutrition needs and preferences according to their age, sex, ethnicity, height, weight, physical activity level, and so on and may benefit from more personalized dietary recommendations.<br><strong> Goals of the Study</strong><br>Our goal is therefore to provide the basis for an algorithm that identifies which group of foods one should consume solely based on their nutrition needs. If we could classify foods into groups that share similar nutrient profiles, it will allow people to customize and plan out everyday meals according to individual nutrition needs and preferences.<br><strong>Data description</strong><br>Data will be pulled from the Canadian Nutrition File. The datasets contain variables related to three main categories: food characteristics, nutrients, and metrics. The food characteristics variable include an ID code for a specific food item, food group, description and source. Nutrients include an ID code for each type of nutrient, the type of nutrients contained and a measure of each component in a food item. Finally, metrics includes data such as portion size conversion factors, percentage of inedible portion and cooking loses by food item. We will combine these datasets into a single one to serve as the basis for our algorithm.<br><strong>Analytic Plans</strong><br>First, we will run spectrum clustering to see which foods are grouped together based on their nutrient profile. In doing so, we want to find out whether we can find clear-cut clusters of foods based on nutrition profile regardless of actual food group assignment. Our grouping results can be used to find the list of foods that contain the best combination of nutrients based on a person’s dietary needs. This improves upon diets that try to rely on the actual food grouping, since our grouping result will be a better representation of the nutrient profile of a set of foods than the food group label, which may be determined arbitrarily.<br>Second, we will test whether actual food groups do share a similar nutrient profile. In other words, we will investigate how well we can predict the group that a certain food actually falls into, given its nutrient profile. We are going to focus on the two largest food groups and choose a subset of foods that fall into either of the two groups. The subsetted data will be split into training and validation data. We will run LASSO using the training data to identify the most important nutrients in predicting food groups and then run logistic regression with food group as the dependent variable. We will then use the validation data to test the robustness of the logistic regression model in predicting food group."
  },
  {
    "id": 6,
    "speakers": [
      {
        "name": "Ying Dai",
        "class": "",
        "pic": null,
        "linkedin" : ""
      },
      {
        "name": "Jonathan Sun",
        "class": "",
        "pic": null
      },
      {
        "name": "Jiayi Zhang",
        "class": "",
        "pic": null
      }
    ],
    "time": "",
    "title": "#StopAsianHate",
    "abstract": "Asian American hate crimes has been on the rise since the beginning of covid-19 with rhetoric such as Kung-flu and the China virus. Due to the current social climate, twitter data using the search term Asian American is more crucial than ever to understand how Asian Americans are framed within the broader racial context of the United States. As such, the purposes of our final project are to 1) explore how Asian Americans are framed and described in social media during the COVID-19 pandemic; and 2) predict what common words are most likely to increase the likelihood of a tweet going viral on the topics around Asian Americans. Using data from Asian-American related tweets over the past five-months Twitter data between November 30th, 2020 to April 1st, 2021 were collected using a tool called “ If This, Then That (IFTTT)” (https://ifttt.com/). We propose to use text mining, lasso, logistic regression, and random forest to conduct our analysis. Specifically using LASSO and logistic regression to build a “best” model to predict a tweet gets viral and builds a separate model using random forest and other approaches to compare which approach works best."
  },
  {
    "id": 7,
    "speakers": [
      {
        "name": "Caroline Harrison",
        "class": "",
        "pic": null,
        "linkedin" : ""
      },
      {
        "name": "Yiran Chen",
        "class": "",
        "pic": null
      }
    ],
    "time": "",
    "title": "COVID-19 Misinformation",
    "abstract": "Over the past several years there has been a major shift in where most people read and learn about the news away from trusted news outlets and towards social media platforms, such as Twitter and Facebook. At the same time, the rise and spread of misinformation on those platforms has grown as well, which has created concerns about the impact reading and sharing fake news has on people. The culmination of these two trends at the beginning of the Covid-19 pandemic in March 2020 resulted in the massive spread of misinformation and fake news across social media platforms, which altered the perception of the severity of the pandemic and negatively impacted the public health measures being enacted to try and slow the spread of the disease. The dataset for this project came from a CodaLab competition and contains over 10,000 pre-labeled tweets and social media posts in English that contain real or fake news and information about the covid-19 pandemic. We aim to perform sentiment analysis and build classification models that can correctly identify real and fake tweets and could be used to slow the spread of misinformation about the pandemic on social media, and provide guidance to social media users as to whether a post contains real or fake information."
  },
  {
    "id": 8,
    "speakers": [
      {
        "name": "Jonerik Blank",
        "class": "WG21",
        "pic": "Jonerik_Blank.jpg",
        "linkedin" : "www.linkedin.com/in/jonerikblank"
      },
      {
        "name": "Scott Yang",
        "class": "WG21 G21",
        "pic": "Scott_Yang.jpg"
      },
      {
        "name": "Peter Zhang",
        "class": "W23",
        "pic": null
      }
    ],
    "time": "",
    "title": "Welcome to the jungle: Making sense of Animal Crossing Reviews",
    "abstract": "Animal Crossing New Horizons is the latest game in a long standing Nintendo Series. Released near the start of many COVID-19 lockdowns, it offered an outlet for players suddenly stuck indoors and served as a symbol of the video game industry’s continued rise. Even before COVID, the video game industry, worth $60 billion in 2019, had grown over the last 25 years at an annual rate of 9-15% earning while revenues 5x that of the music industry and about the same as the film industry.1 These Animal Crossing players added their reviews and thoughts (eWOM)* to those published by traditional critics such as IGN or Gamespot. A recent academic study examining the effects of extrinsic and intrinsic cues on daily video game sales found that laudatory eWOM had more than twice the positive effect over the next most consequential cue type.2 Clearly managing player and reviewer critical reception is vital to a game’s market performance. Leveraging a dataset comprised of both critical and user reviews, we examine how various NLP methods can be used to produce accurate predictive models for both positive and negative reviews using specific words. Developers could, in the future, use these models during playtesting to gauge the likely critical reception of their title.\n\n*eWOM – electronic word of mouth\n\n1. Merchand, André, and Thorsten Hennig-Thurau. \"Value Creation in the Video Game Industry: Industry Economics, Consumer Benefits, and Research Opportunities.\" Journal of Interactive Marketing 27 (2013): 142. Accessed April 21, 2021. DOI: 10.1016/j.intmar.2013.05.001.\n2. Choi, Hoon S. et al. \"The effect of intrinsic and extrinsic quality cues of digital video games on sales: An empirical investigation.\" Decision Support Systems 106 (February 2018): 92-93. "
  },
  {
    "id": 9,
    "speakers": [
      {
        "name": "Norman Chen",
        "class": "",
        "pic": null,
        "linkedin" : ""
      },
      {
        "name": "Cathy Chen",
        "class": "",
        "pic": null
      },
      {
        "name": "Andrew Yu",
        "class": "",
        "pic": null
      }
    ],
    "time": "",
    "title": "The Startup Sub Sorter",
    "abstract": "In the space of content creation, subscribers rule above all else. With the recent wave of venture capital and entrepreneurship-focused content from newsletters to books to podcasts, the startup content creation space is rapidly maturing. Content creators will need to more heavily compete for advertisement deals and sponsorships, and in order to do so, it is imperative that they understand who their subscribers are. However, content delivery platforms like Substack and Anchor aren’t equipped with the right tools to properly break down the audience for entrepreneurship content. This makes it hard for creators to curate good content for their audience, and it’s doubly difficult to secure advertisement deals if they can only guess at their audience base. We hope to build the Startup Sub Sifter: a scalable model that can quickly and accurately categorize subscribers of entrepreneurship-focused content into useful categories (e.g. investors vs startup operators vs students). Ideally, this tool should be (1) cost-free considering that many creators are not yet monetized, (2) relatively accurate and granular, and (3) repeatable OR continuously and automatically updated. "
  }
]