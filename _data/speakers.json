[
  {
    "name": "Susan Murphy",
    "year": "2024",
    "affiliation": "Harvard University",
    "url": "http://people.seas.harvard.edu/~samurphy/",
    "bio": "Susan Murphy is Professor of Statistics at Harvard University, Radcliffe Alumnae Professor at the Radcliffe Institute, Harvard University, and Professor of Computer Science at the Harvard John A. Paulson School of Engineering and Applied Sciences. Her lab works on clinical trial designs and online learning algorithms in sequential decision making, in particular in the area of digital health. She developed the micro-randomized trial for use in constructing mobile health interventions which is in use across a broad range of health- related areas. She is a 2013 MacArthur Fellow, a member of the National Academy of Sciences and the National Academy of Medicine, both of the US National Academies. She is a Past-President of the Institute of Mathematical Statistics and of the Bernoulli Society and a former editor of the Annals of Statistics. She is a prior recipient of the RA Fisher Award from the Committee of Presidents of Statistical Societies (COPSS) and the Guy Medal in Silver from the Royal Statistical Society.",
    "lectures": [
    ]
  },
  {
    "name": "David Donoho",
    "year": "2023",
    "affiliation": "Stanford University",
    "url": "https://web.stanford.edu/dept/statistics/cgi-bin/donoho/",
    "bio": "David Donoho has studied the exploitation of sparse signals in signal recovery, including for denoising, superresolution, and solution of underdetermined equations. His research with collaborators showed that ell-1 penalization was an effective and even optimal way to exploit sparsity of the object to be recovered. He coined the notion of compressed sensing which has impacted many scientific and technical fields, including magnetic resonance imaging in medicine, where it has been implemented in FDA-approved medical imaging protocols and is already used in millions of actual patient MRIs.<br><br> In recent years David and his postdocs and students have been studying large-scale covariance matrix estimation, large-scale matrix denoising, detection of rare and weak signals among many pure noise non-signals, compressed sensing and related scientific imaging problems, and most recently, empirical deep learning.",
    "lectures": [
      {
        "id": 1, 
        "title": "Data Science vs Statistics",
        "date": "April 23, 2024",
        "abstract": "A conventional narrative tells us that Data Science is just a rebranding of traditional statistics. This talk explores the idea that there is a “Data Science” mindset derived from modern digital life, and a “Statistics Mindset” derived from long intellectual tradition. These mindsets breed two completely different mental realities ie. different thoughts we can hold in mind and pay attention to. Because of this, there is a gaping divide between what residents of each mental reality can focus upon, produce and value. Downstream of this, two completely separate discourses and cultures are developing.<br><br> In our view, the two sides don’t properly understand that there are two sides, and that severe challenges are caused by this split. This shows up when each camp reflects on the other in frequent frustration, pointless conversations and negative emotions. This can be seen in situations where teams from each side of the divide both do research about the same topic, and try to engage with each other's research. <br><br> It is important for the statistics tradition to drop the blinders and see the situation clearly, to finally benefit from the existence of the data science reality. Similarly, data science could make better progress by clearly understanding what the statistics tradition can offer.<br><br> Finally, the biggest opportunities will come for those who can become bicultural. The talk should be accessible to a broader audience. This is joint work with Matan Gavish (Hebrew University CS)."
      },
      {
        "id": 2, 
        "title": "Widespread Panic over Model Collapse",
        "date": "April 24, 2024",
        "abstract": "Modern ML systems are extraordinarily data hungry; and some major commercial players are said to now be using synthetic data to train their most ambitious ML systems. Also, AI-generated data will soon flood the internet, perhaps to the point where most available data are synthetic. Recently ML research has started to confront the larger issues that synthetic data might pose, including a future where most or all of the data available for an ML training are synthetic. A number of ML papers became prominent after promoting the idea of “model collapse”, the “curse of recursion”, “model autophagy disorder”. Featuring experiments and some very basic theoretical argumentation they promoted a storyline where successive recycling of purely synthetic data led to model degeneration. <br><br>In contrast, Mathematical Scientists have looked at the same setting as the ML researchers, and developed a more balanced view of the situation, depending on the synthetic data use case, no such collapse occurs. Empirical work with canonical LLMs and diffusion models confirms the absence of collapse, in the recommended use case. <br><br>I will review the setting, the narrative promoting to panic and counter narratives leading to a calmer view. The talk should be accessible to a broader audience, as most of the research in this area consists of analysis at the statistics undergraduate major or master’s level — although one could transplant the basic questions into fancier settings if one liked. <br><br> This is joint work with Apratim Dey of Stanford Statistics and a CS team at Stanford."
      },
      {
        "id": 3, 
        "title": "Optimal Vector Sensing by Stein Shrinkage",
        "date": "April 25, 2024",
        "abstract": "We review some Compressed Sensing theory through the lens of Approximate Message Passing, and minimax decision theory concerning shrinkage estimates. AMP powers up a simple estimator for an elementary problem into a procedure for a drastically more ambitious problem of compressed sensing. In this case the simple estimator is James-Stein shrinkage and we use it to construct a procedure of multiple measurement vector compressed sensing. <br><br> We discuss the State Evolution analysis of this procedure, prove and empirically verify all the predictions of state evolution, and so on. We discuss the state evolution theory of Compressed Sensing in the sense of sparsity-undersampling phase diagram. In the large dimensional limit both in system size and vector size, with Gaussian sensing matrices, James-Stein has a theoretically unimprovable phase diagram and empirically works near-optimally even in low vector dimensions. In particular this is far better than the convex optimization approaches. <br><br> This is joint work with Apratim Dey (Stanford Statistics)."
      }
    ]
  },
  {
    "name": "Iain Johnstone",
    "affiliation": "Stanford University",
    "bio": "Dr. Johnstone is a statistician with research interests in statistical decision theory and wavelet-like methods (and their uses) in estimation theory, asymptotics and application areas such as statistical inverse problems and statistical signal processing. Other interests include simulation methodology, volume tests of significance, hazard rate estimation and maximum entropy methods.",
    "year": "2022",
    "url": "https://imjohnstone.su.domains",
    "lectures": [
      {
        "id": 1, 
        "title": "Estimating Sparse Eigenstructure for High Dimensional Data",
        "date": "November 16, 2022",
        "abstract": "When data is high dimensional, widely used multivariate methods such as principal component analysis can behave in unexpected ways. Upward bias in sample eigenvalues and inconsistency of sample eigenvectors are among the new phenomena that appear. In this expository overview talk, I will try to use (amateur!) graphics and heuristic arguments to explain how these phenomena arise, and some of the things that can be done in response."
      },
      {
        "id": 2, 
        "title": "Likelihood Ratios and a Transition from Gaussian to Tracy-Widom",
        "date": "November 17, 2022",
        "abstract": "The Tracy-Widom distribution has found broad use in statistical theory and application. We first review some of this, focusing first on Principal Components Analysis and the `spiked model`. When the spike signal is below the Baik-Ben Arous-Peche threshold, likelihood ratio tests for presence of a spike are more efficient than the largest eigenvalue in many settings of multivariate statistics. In recent work in a spiked Wigner model with Egor Klochkov, Alexei Onatski and Damian Pavlyshyn, we study the likelihood ratio test in the transition zone around the BBP threshold, making use of a connection with the spherical Sherrington-Kirkpatrick model and establishing a conjecture of Baik and Lee."
      },
      {
        "id": 3, 
        "title": "Expectation Propagation in Mixed Models",
        "date": "November 18, 2022",
        "abstract": "Matt Wand and colleagues have recently shown that the machine learning technique of expectation propagation (EP) yields state of the art estimation of parameters in generalized linear mixed models. We review this work before asking: are the EP estimators asymptotically efficient? The problem becomes one of defining an appropriate objective function that captures the EP iteration and approximates maximum likelihood well enough to inherit its efficiency. Joint work with the late Peter Hall, Song Mei, and Matt Wand."
      }
    ]
  },
  {
    "name": "Jim Berger",
    "bio": "Berger received his Ph.D. degree in mathematics from Cornell University in 1974. He was a faculty member in the Department of Statistics at Purdue University until 1997, at which time he moved to the Institute of Statistics and Decision Sciences (now the Department of Statistical Science) at Duke University, where he is currently the Arts and Sciences Professor of Statistics. He was the founding director of the Statistical and Applied Mathematical Sciences Institute, serving from 2002-2010.<br><br>Berger was president of the Institute of Mathematical Statistics during 1995-1996, chair of the Section on Bayesian Statistical Science of the American Statistical Association in 1995, and president of the International Society for Bayesian Analysis during 2004. He has been involved with numerous editorial activities, including co-editorship of the Annals of Statistics during the period 1998-2000 and being a founding editor of the Journal on Uncertainty Quantification, serving from 2012-2015.<br><br> Among the awards and honors Berger has received are Guggenheim and Sloan Fellowships, the COPSS President's Award in 1985, the Sigma Xi Research Award at Purdue University for contribution of the year to science in 1993, the COPSS Fisher Lecturer in 2001, the Wald Lecturer of the IMS in 2007 and the Wilks Award from the ASA in 2015. He was elected as foreign member of the Spanish Real Academia de Ciencias in 2002, elected to the USA National Academy of Sciences in 2003, was awarded an honorary Doctor of Science degree from Purdue University in 2004, and became an Honorary Professor at East China Normal University in 2011.<br><br> Berger's research has primarily been in Bayesian statistics, foundations of statistics, statistical decision theory, simulation, model selection, and various interdisciplinary areas of science and industry, including astronomy, geophysics, medicine, and validation of complex computer models. He has supervised 36 Ph.D. dissertations, published over 190 papers and has written or edited 16 books or special volumes.",
    "affiliation": "Duke University",
    "year": "2021",
    "url": "http://www2.stat.duke.edu/~berger/",
    "lectures": [
      {
        "id": 1, 
        "title": "Emulation of Computer Models with Massive Output",
        "date": "September 8, 2021",
        "abstract": "Often computer models yield massive output; e.g., a weather model will yield the predicted temperature over a huge grid of points in space and time. Emulation of a computer model is the process of finding an approximation to the computer model that is much faster to run than the computer model itself (which can often take hours or days for a single run). Many successful emulation approaches are statistical in nature. We discuss one such approach – the construction of independent parallel emulators at each grid point – with the emulators being developed through Gaussian processes. The computational simplicity with which this approach can be implemented will be highlighted and the surprising fact that one can ignore spatial structure in the massive output will be explained. All results will be illustrated with a computer model of volcanic pyroclastic flow, the goal being the prediction of hazard probabilities near active volcanoes."
      },
      {
        "id": 2, 
        "title": "Four Types of Frequentism and Their Interplay with Bayesianism",
        "date": "September 9, 2021",
        "abstract": "The majority of statisticians and scientists who use statistics declare themselves to be frequentists, but they typically mean very different things by this declaration. The purpose of this talk is to highlight the major different types of frequentists and to indicate which are compatible with Bayesianism and which are not. The focus is on evaluating common statistical procedures from the different perspectives, primarily from three unconditional frequentist perspectives."
      },
      {
        "id": 3, 
        "title": "The Conditional Frequentist Principle and Data Dependent Error Probabilities",
        "date": "September 10, 2021",
        "abstract": "The majority of statisticians and scientists who use statistics declare themselves to be frequentists, but they typically mean very different things by this declaration. This talk is a continuation of Talk 2 on the subject, with the focus shifting to the conditional frequentist perspective, rather than the unconditional frequentist perspective; it is not necessary to have been at the previous talk to follow this one. Larry Brown’s significant work in this area will be highlighted."
      }
    ]
  } 
]
